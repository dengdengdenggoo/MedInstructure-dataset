# MedLLM Framework: Large Language Framework for Medical Question Answering
Large language models (LLMs) hold great potential in medical question answering (QA) tasks, but their performance in specific domains remains limited. To address this issue, we propose MedLLM framework, a cost-effective medical QA framework based on LoRA, LoRA+, DoRA, and PiSSA. Training data consists of two medical QA datasets, one of which is self-built, covering a variety of clinical scenarios and question types. Experimental results demonstrate that MedLLM outperforms mainstream large language models of similar size by approximately 60\% on metrics such as Blue-4, Rouge-1, Rouge-2, and Rouge-L, achieving higher answer accuracy and generation quality. These results validate the necessity and effectiveness of supervised fine-tuning for specific domain tasks. This study provides valuable insights into data construction, training strategies, and performance evaluation of LLMs in medical QA, laying the foundation for their application in clinical decision support and medical knowledge services. The dataset will be released after the paper is published.
